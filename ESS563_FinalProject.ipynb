{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2266193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy import Stream, UTCDateTime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.collections import LineCollection, PolyCollection\n",
    "import importlib\n",
    "from io import StringIO\n",
    "import io\n",
    "import picking_funcs_new as pf\n",
    "from picking_funcs_new import (\n",
    "    arrakis_read, das_read,\n",
    "    arrakis_preprocess, das_preprocess,\n",
    "    comb_trigger_simple, das_trigger_simple, AdvOpt, VetoOpt,\n",
    ")\n",
    "# importlib.reload(pf)\n",
    "from typing import Sequence, Tuple, Union, Optional, Literal,List,Dict, Iterable,Mapping,Any\n",
    "from pathlib import Path\n",
    "\n",
    "import textwrap\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
    "from scipy.signal import find_peaks, hilbert,correlate\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timezone, UTC\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from scipy.interpolate import pchip_interpolate\n",
    "from scipy.optimize import curve_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ef060",
   "metadata": {},
   "source": [
    "# The first section in this notebook can not be added until the dataset is made publically available in February 2026. At that time this notebook will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_traces(stream, df_picks, event_id, \n",
    "                         pre_pick=0.02, post_pick=0.06):\n",
    "    \"\"\"\n",
    "    Extract geophone traces for a single event, windowing each trace \n",
    "    individually around its own P arrival pick.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stream : obspy.Stream\n",
    "        Full stream containing all traces (geophones and DAS)\n",
    "    df_picks : pandas.DataFrame\n",
    "        DataFrame with columns: 'eventid', 'station', 'pick', 'origin'\n",
    "        Times are stored as strings\n",
    "    event_id : str or int\n",
    "        Unique event identifier to extract\n",
    "    pre_pick : float\n",
    "        Time in seconds BEFORE each station's P pick to include\n",
    "    post_pick : float\n",
    "        Time in seconds AFTER each station's P pick to include\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    event_stream : obspy.Stream\n",
    "        Stream containing windowed geophone traces, each centered on its own pick\n",
    "    pick_times : dict\n",
    "        Dictionary mapping station codes to their P pick times (UTCDateTime)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter picks for this specific event\n",
    "    event_picks = df_picks[df_picks['eventid'] == event_id].copy()\n",
    "    \n",
    "    if len(event_picks) == 0:\n",
    "        raise ValueError(f\"No picks found for event_id: {event_id}\")\n",
    "    \n",
    "    # Filter for geophone stations only (3-character station names)\n",
    "    event_picks = event_picks[event_picks['station'].str.len() == 3]\n",
    "    \n",
    "    # Remove station 5-1 from picks (poor data quality)\n",
    "    event_picks = event_picks[event_picks['station'] != '5-1']\n",
    "    \n",
    "    if len(event_picks) == 0:\n",
    "        raise ValueError(f\"No geophone picks found for event_id: {event_id}\")\n",
    "    \n",
    "    # Get list of stations with picks for this event\n",
    "    stations_with_picks = event_picks['station'].unique().tolist()\n",
    "    \n",
    "    print(f\"Event {event_id}:\")\n",
    "    print(f\"  Number of geophone stations with picks: {len(stations_with_picks)}\")\n",
    "    print(f\"  Window: {pre_pick} s before to {post_pick} s after each pick\")\n",
    "    print(f\"  Total window length: {pre_pick + post_pick} s\")\n",
    "    \n",
    "    # Create dictionary of pick times\n",
    "    pick_times = {}\n",
    "    for _, row in event_picks.iterrows():\n",
    "        station = row['station']\n",
    "        pick_utc = UTCDateTime(row['pick'])\n",
    "        pick_times[station] = pick_utc\n",
    "    \n",
    "    # Extract and window each trace individually around its own pick\n",
    "    event_stream = obspy.Stream()\n",
    "    \n",
    "    for station in stations_with_picks:\n",
    "        # Get the pick time for this station\n",
    "        pick_time = pick_times[station]\n",
    "        \n",
    "        # Define window for THIS station\n",
    "        window_start = pick_time - pre_pick\n",
    "        window_end = pick_time + post_pick\n",
    "        \n",
    "        # Try to get the trace for this station\n",
    "        tr_select = stream.select(network='SQ', station=station)\n",
    "        \n",
    "        if len(tr_select) == 0:\n",
    "            print(f\"  Warning: No trace found for station {station}\")\n",
    "            continue\n",
    "        \n",
    "        if len(tr_select) > 1:\n",
    "            print(f\"  Warning: Multiple traces found for station {station}, using first\")\n",
    "        \n",
    "        # Get the trace and window it\n",
    "        tr = tr_select[0].copy()\n",
    "        tr_windowed = tr.slice(starttime=window_start, endtime=window_end)\n",
    "        \n",
    "        # Check if we got valid data\n",
    "        if tr_windowed.stats.npts == 0:\n",
    "            print(f\"  Warning: No data in window for station {station}\")\n",
    "            continue\n",
    "        \n",
    "        event_stream.append(tr_windowed)\n",
    "    \n",
    "    print(f\"  Extracted {len(event_stream)} windowed traces\")\n",
    "    \n",
    "    return event_stream, pick_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: extract windows for one early event\n",
    "first_event_id = df_picks['eventid'].iloc[9420]  # Or however you identify early vs late events\n",
    "# late_origin = df_picks['origin'].iloc[10921]   # Or however you identify late event\n",
    "# early_origin = df_picks['origin'].iloc[9753]  # Or however you identify early vs late events\n",
    "print(f\"Testing with event: {first_event_id}\\n\")\n",
    "\n",
    "# Extract traces for this event\n",
    "event_stream, pick_times = extract_event_traces(\n",
    "    comb_stZ, \n",
    "    df_picks, \n",
    "    first_event_id,\n",
    "    pre_pick=0.02,\n",
    "    post_pick=0.06\n",
    ")\n",
    "# Quick diagnostic - add this after the function returns\n",
    "stations_in_stream = set([tr.stats.station for tr in event_stream])\n",
    "stations_in_picks = set(pick_times.keys())\n",
    "\n",
    "missing_pick = stations_in_stream - stations_in_picks\n",
    "extra_pick = stations_in_picks - stations_in_stream\n",
    "\n",
    "if missing_pick:\n",
    "    print(f\"  Stations in stream WITHOUT picks: {missing_pick}\")\n",
    "if extra_pick:\n",
    "    print(f\"  Stations with picks NOT in stream: {extra_pick}\")\n",
    "# Print trace information\n",
    "print(f\"\\nTraces in event_stream:\")\n",
    "for tr in event_stream:\n",
    "    print(f\"  {tr.stats.station}: {tr.stats.npts} samples, \"\n",
    "          f\"dt={tr.stats.delta:.4f} s\")\n",
    "\n",
    "# Print pick times\n",
    "print(f\"\\nPick times:\")\n",
    "for station, pick_time in pick_times.items():\n",
    "    print(f\"  {station}: {pick_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_waveform(event_stream, pick_times, \n",
    "                              snr_window_signal=0.02, snr_window_noise=0.02,\n",
    "                              cc_window_length=0.03, max_lag_sec=0.01, \n",
    "                              normalize=True):\n",
    "    \"\"\"\n",
    "    Create a reference waveform by aligning and stacking traces using cross-correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_stream : obspy.Stream\n",
    "        Windowed stream for a single event (each trace windowed around its own pick)\n",
    "    pick_times : dict\n",
    "        Dictionary mapping station codes to P pick times (UTCDateTime)\n",
    "    snr_window_signal : float\n",
    "        Time window (s) after pick to calculate signal power\n",
    "    snr_window_noise : float\n",
    "        Time window (s) before pick to calculate noise power\n",
    "    cc_window_length : float\n",
    "        Length of window (s) to use for cross-correlation alignment\n",
    "        Should be long enough to capture first arrival pulse only\n",
    "    max_lag_sec : float\n",
    "        Maximum allowed lag (in seconds) for cross-correlation alignment\n",
    "    normalize : bool\n",
    "        If True, normalize each trace before stacking\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    reference_trace : obspy.Trace\n",
    "        Stacked and aligned reference waveform\n",
    "    aligned_stream : obspy.Stream\n",
    "        Stream of aligned individual traces (for QC)\n",
    "    snr_dict : dict\n",
    "        SNR values for each station\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(event_stream) == 0:\n",
    "        raise ValueError(\"Empty stream provided\")\n",
    "    \n",
    "    # Get sampling rate (assume all traces have same rate)\n",
    "    sampling_rate = event_stream[0].stats.sampling_rate\n",
    "    max_lag_samples = int(max_lag_sec * sampling_rate)\n",
    "    cc_window_samples = int(cc_window_length * sampling_rate)\n",
    "    \n",
    "    print(f\"\\n  Max allowed lag: {max_lag_sec} s ({max_lag_samples} samples)\")\n",
    "    print(f\"  Cross-correlation window: {cc_window_length} s ({cc_window_samples} samples)\")\n",
    "    \n",
    "    # Calculate SNR for each trace\n",
    "    snr_dict = {}\n",
    "    valid_traces = []\n",
    "    \n",
    "    # For each trace, the pick should be at the start of the trace + pre_pick time\n",
    "    # We need to figure out where the pick is in each trace\n",
    "    for tr in event_stream:\n",
    "        station = tr.stats.station\n",
    "        \n",
    "        # Skip if we don't have a pick for this station\n",
    "        if station not in pick_times:\n",
    "            print(f\"  Warning: No pick time for station {station}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get pick time relative to trace start\n",
    "        pick_time = pick_times[station]\n",
    "        pick_sample = int((pick_time - tr.stats.starttime) * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Calculate SNR\n",
    "        noise_samples = int(snr_window_noise * tr.stats.sampling_rate)\n",
    "        signal_samples = int(snr_window_signal * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Make sure we have enough samples\n",
    "        if pick_sample < noise_samples or pick_sample + signal_samples >= tr.stats.npts:\n",
    "            print(f\"  Warning: Pick too close to edge for {station}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Noise: window before pick\n",
    "        noise = tr.data[pick_sample - noise_samples : pick_sample]\n",
    "        # Signal: window after pick\n",
    "        signal_win = tr.data[pick_sample : pick_sample + signal_samples]\n",
    "        \n",
    "        # Calculate RMS for signal and noise\n",
    "        noise_rms = np.sqrt(np.mean(noise**2))\n",
    "        signal_rms = np.sqrt(np.mean(signal_win**2))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if noise_rms > 0:\n",
    "            snr = signal_rms / noise_rms\n",
    "        else:\n",
    "            snr = 0\n",
    "            \n",
    "        snr_dict[station] = snr\n",
    "        valid_traces.append(tr.copy())\n",
    "    \n",
    "    if len(valid_traces) == 0:\n",
    "        raise ValueError(\"No valid traces with picks\")\n",
    "    \n",
    "    print(f\"\\n  Valid traces for stacking: {len(valid_traces)}\")\n",
    "    \n",
    "    # Select pilot trace (highest SNR)\n",
    "    pilot_station = max(snr_dict, key=snr_dict.get)\n",
    "    pilot_trace = [tr for tr in valid_traces if tr.stats.station == pilot_station][0]\n",
    "    pilot_pick_sample = int((pick_times[pilot_station] - pilot_trace.stats.starttime) \n",
    "                           * pilot_trace.stats.sampling_rate)\n",
    "    \n",
    "    print(f\"  Pilot trace: {pilot_station} (SNR = {snr_dict[pilot_station]:.1f})\")\n",
    "    \n",
    "    # Extract window around pick from pilot for cross-correlation template\n",
    "    # Window centered on pick, extending cc_window_length/2 on each side\n",
    "    cc_half_window = cc_window_samples // 2\n",
    "    pilot_cc_start = max(0, pilot_pick_sample - cc_half_window)\n",
    "    pilot_cc_end = min(len(pilot_trace.data), pilot_pick_sample + cc_half_window)\n",
    "    pilot_cc_window = pilot_trace.data[pilot_cc_start:pilot_cc_end]\n",
    "    \n",
    "    # Align all traces to pilot using cross-correlation with constrained lag\n",
    "    aligned_stream = obspy.Stream()\n",
    "    aligned_data = []\n",
    "    \n",
    "    for tr in valid_traces:\n",
    "        station = tr.stats.station\n",
    "        \n",
    "        # Get pick sample for this trace\n",
    "        pick_sample = int((pick_times[station] - tr.stats.starttime) \n",
    "                         * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Extract window around pick for cross-correlation\n",
    "        tr_cc_start = max(0, pick_sample - cc_half_window)\n",
    "        tr_cc_end = min(len(tr.data), pick_sample + cc_half_window)\n",
    "        tr_cc_window = tr.data[tr_cc_start:tr_cc_end]\n",
    "        \n",
    "        # Cross-correlate ONLY the windowed segments around the picks\n",
    "        correlation = correlate(tr_cc_window, pilot_cc_window, mode='same')\n",
    "        \n",
    "        # Restrict search to +/- max_lag_samples around zero lag\n",
    "        center = len(correlation) // 2\n",
    "        search_start = max(0, center - max_lag_samples)\n",
    "        search_end = min(len(correlation), center + max_lag_samples + 1)\n",
    "        \n",
    "        # Find best lag within constrained window\n",
    "        local_max_idx = np.argmax(correlation[search_start:search_end])\n",
    "        lag_sample = (search_start + local_max_idx) - center\n",
    "        \n",
    "        # Get correlation coefficient at this lag\n",
    "        cc_max = correlation[search_start + local_max_idx]\n",
    "        cc_norm = cc_max / (np.linalg.norm(tr_cc_window) * np.linalg.norm(pilot_cc_window))\n",
    "        \n",
    "        print(f\"  {station}: SNR={snr_dict[station]:.1f}, lag={lag_sample} samples ({lag_sample/sampling_rate*1000:.1f} ms), CC={cc_norm:.3f}\")\n",
    "        \n",
    "        # Shift the ENTIRE trace by the lag determined from the windowed cross-correlation\n",
    "        if lag_sample > 0:\n",
    "            aligned = np.pad(tr.data[lag_sample:], (0, lag_sample), \n",
    "                           mode='constant', constant_values=0)\n",
    "        elif lag_sample < 0:\n",
    "            aligned = np.pad(tr.data[:lag_sample], (-lag_sample, 0), \n",
    "                           mode='constant', constant_values=0)\n",
    "        else:\n",
    "            aligned = tr.data.copy()\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize:\n",
    "            aligned = aligned / np.max(np.abs(aligned))\n",
    "        \n",
    "        # Store aligned trace\n",
    "        tr_aligned = tr.copy()\n",
    "        tr_aligned.data = aligned\n",
    "        aligned_stream.append(tr_aligned)\n",
    "        aligned_data.append(aligned)\n",
    "    \n",
    "    # Stack aligned traces\n",
    "    stacked_data = np.mean(aligned_data, axis=0)\n",
    "    \n",
    "    # Create reference trace (copy stats from pilot)\n",
    "    reference_trace = pilot_trace.copy()\n",
    "    reference_trace.data = stacked_data\n",
    "    reference_trace.stats.station = 'STACK'\n",
    "    \n",
    "    print(f\"\\n  Reference waveform created from {len(aligned_data)} traces\")\n",
    "    \n",
    "    return reference_trace, aligned_stream, snr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_trace, aligned_stream, snr_dict = create_reference_waveform(\n",
    "    event_stream, \n",
    "    pick_times,\n",
    "    cc_window_length=0.03,  # Only use 15 samples around pick for alignment\n",
    "    max_lag_sec=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913735a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot traces BEFORE alignment (just the raw windowed traces)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot raw windowed traces (before alignment)\n",
    "for tr in event_stream:\n",
    "    axes[0].plot(tr.times(), tr.data / np.max(np.abs(tr.data)), \n",
    "                 alpha=0.5, linewidth=0.5)\n",
    "    # Mark the pick position (should be at pre_pick seconds)\n",
    "axes[0].axvline(x=0.02, color='r', linestyle='--', label='Expected pick position')\n",
    "axes[0].set_title('Raw Windowed Traces (Before Alignment)')\n",
    "axes[0].set_ylabel('Normalized Amplitude')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot aligned traces\n",
    "for tr in aligned_stream:\n",
    "    axes[1].plot(tr.times(), tr.data, alpha=0.5, linewidth=0.5)\n",
    "axes[1].set_title('After Cross-Correlation Alignment')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Normalized Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_waveform(event_stream, pick_times, \n",
    "                              snr_window_signal=0.02, snr_window_noise=0.02,\n",
    "                              cc_window_length=0.03, max_lag_sec=0.01,\n",
    "                              min_cc_threshold=0.7, normalize=True):\n",
    "    \"\"\"\n",
    "    Create a reference waveform by aligning and stacking traces using cross-correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_stream : obspy.Stream\n",
    "        Windowed stream for a single event (each trace windowed around its own pick)\n",
    "    pick_times : dict\n",
    "        Dictionary mapping station codes to P pick times (UTCDateTime)\n",
    "    snr_window_signal : float\n",
    "        Time window (s) after pick to calculate signal power\n",
    "    snr_window_noise : float\n",
    "        Time window (s) before pick to calculate noise power\n",
    "    cc_window_length : float\n",
    "        Length of window (s) to use for cross-correlation alignment\n",
    "        Should capture ~1-2 cycles of the dominant frequency\n",
    "        Default 0.03 s works well for 10-100 Hz bandpass\n",
    "    max_lag_sec : float\n",
    "        Maximum allowed lag (in seconds) for cross-correlation alignment\n",
    "        Should be fraction of dominant period to avoid cycle skips\n",
    "        Default 0.01 s = half cycle at 50 Hz\n",
    "    min_cc_threshold : float\n",
    "        Minimum correlation coefficient to accept alignment (0-1)\n",
    "    normalize : bool\n",
    "        If True, normalize each trace before stacking\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    reference_trace : obspy.Trace\n",
    "        Stacked and aligned reference waveform\n",
    "    aligned_stream : obspy.Stream\n",
    "        Stream of aligned individual traces (for QC)\n",
    "    snr_dict : dict\n",
    "        SNR values for each station\n",
    "    alignment_quality : dict\n",
    "        Correlation coefficients for each station's alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(event_stream) == 0:\n",
    "        raise ValueError(\"Empty stream provided\")\n",
    "    \n",
    "    # Get sampling rate (assume all traces have same rate)\n",
    "    sampling_rate = event_stream[0].stats.sampling_rate\n",
    "    cc_window_samples = int(cc_window_length * sampling_rate)\n",
    "    max_lag_samples = int(max_lag_sec * sampling_rate)\n",
    "    \n",
    "    print(f\"\\n  CC window: {cc_window_length*1000:.1f} ms ({cc_window_samples} samples)\")\n",
    "    print(f\"  Max lag: {max_lag_sec*1000:.1f} ms ({max_lag_samples} samples)\")\n",
    "    \n",
    "    # Calculate SNR for each trace\n",
    "    snr_dict = {}\n",
    "    valid_traces = []\n",
    "    \n",
    "    for tr in event_stream:\n",
    "        station = tr.stats.station\n",
    "        \n",
    "        # Skip if we don't have a pick for this station\n",
    "        if station not in pick_times:\n",
    "            print(f\"  Warning: No pick time for station {station}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get pick time relative to trace start\n",
    "        pick_time = pick_times[station]\n",
    "        pick_sample = int((pick_time - tr.stats.starttime) * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Calculate SNR\n",
    "        noise_samples = int(snr_window_noise * tr.stats.sampling_rate)\n",
    "        signal_samples = int(snr_window_signal * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Make sure we have enough samples\n",
    "        if pick_sample < noise_samples or pick_sample + signal_samples >= tr.stats.npts:\n",
    "            print(f\"  Warning: Pick too close to edge for {station}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Noise: window before pick\n",
    "        noise = tr.data[pick_sample - noise_samples : pick_sample]\n",
    "        # Signal: window after pick\n",
    "        signal_win = tr.data[pick_sample : pick_sample + signal_samples]\n",
    "        \n",
    "        # Calculate RMS for signal and noise\n",
    "        noise_rms = np.sqrt(np.mean(noise**2))\n",
    "        signal_rms = np.sqrt(np.mean(signal_win**2))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if noise_rms > 0:\n",
    "            snr = signal_rms / noise_rms\n",
    "        else:\n",
    "            snr = 0\n",
    "            \n",
    "        snr_dict[station] = snr\n",
    "        valid_traces.append(tr.copy())\n",
    "    \n",
    "    if len(valid_traces) == 0:\n",
    "        raise ValueError(\"No valid traces with picks\")\n",
    "    \n",
    "    print(f\"  Valid traces for stacking: {len(valid_traces)}\")\n",
    "    \n",
    "    # Select pilot trace (highest SNR)\n",
    "    pilot_station = max(snr_dict, key=snr_dict.get)\n",
    "    pilot_trace = [tr for tr in valid_traces if tr.stats.station == pilot_station][0]\n",
    "    pilot_pick_sample = int((pick_times[pilot_station] - pilot_trace.stats.starttime) \n",
    "                           * pilot_trace.stats.sampling_rate)\n",
    "    \n",
    "    print(f\"  Pilot trace: {pilot_station} (SNR = {snr_dict[pilot_station]:.1f})\")\n",
    "    \n",
    "    # Extract window around pick from pilot for cross-correlation template\n",
    "    cc_half_window = cc_window_samples // 2\n",
    "    pilot_cc_start = max(0, pilot_pick_sample - cc_half_window)\n",
    "    pilot_cc_end = min(len(pilot_trace.data), pilot_pick_sample + cc_half_window)\n",
    "    pilot_cc_window = pilot_trace.data[pilot_cc_start:pilot_cc_end]\n",
    "    \n",
    "    # Align all traces to pilot using cross-correlation\n",
    "    aligned_stream = obspy.Stream()\n",
    "    aligned_data = []\n",
    "    alignment_quality = {}\n",
    "    low_cc_stations = []\n",
    "    \n",
    "    for tr in valid_traces:\n",
    "        station = tr.stats.station\n",
    "        \n",
    "        # Get pick sample for this trace\n",
    "        pick_sample = int((pick_times[station] - tr.stats.starttime) \n",
    "                         * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Extract window around pick for cross-correlation\n",
    "        tr_cc_start = max(0, pick_sample - cc_half_window)\n",
    "        tr_cc_end = min(len(tr.data), pick_sample + cc_half_window)\n",
    "        tr_cc_window = tr.data[tr_cc_start:tr_cc_end]\n",
    "        \n",
    "        # Cross-correlate ONLY the windowed segments around the picks\n",
    "        correlation = correlate(tr_cc_window, pilot_cc_window, mode='same')\n",
    "        \n",
    "        # Restrict search to +/- max_lag_samples around zero lag\n",
    "        center = len(correlation) // 2\n",
    "        search_start = max(0, center - max_lag_samples)\n",
    "        search_end = min(len(correlation), center + max_lag_samples + 1)\n",
    "        \n",
    "        # Find best lag within constrained window\n",
    "        local_max_idx = np.argmax(correlation[search_start:search_end])\n",
    "        lag_sample = (search_start + local_max_idx) - center\n",
    "        \n",
    "        # Calculate normalized correlation coefficient\n",
    "        cc_max = correlation[search_start + local_max_idx]\n",
    "        cc_norm = cc_max / (np.linalg.norm(tr_cc_window) * np.linalg.norm(pilot_cc_window))\n",
    "        alignment_quality[station] = cc_norm\n",
    "        \n",
    "        # Flag low correlation coefficients\n",
    "        if cc_norm < min_cc_threshold:\n",
    "            low_cc_stations.append(station)\n",
    "            flag = \" [LOW CC!]\"\n",
    "        else:\n",
    "            flag = \"\"\n",
    "        \n",
    "        print(f\"  {station}: SNR={snr_dict[station]:.1f}, lag={lag_sample} samples ({lag_sample/sampling_rate*1000:.1f} ms), CC={cc_norm:.3f}{flag}\")\n",
    "        \n",
    "        # Shift the ENTIRE trace by the lag\n",
    "        if lag_sample > 0:\n",
    "            aligned = np.pad(tr.data[lag_sample:], (0, lag_sample), \n",
    "                           mode='constant', constant_values=0)\n",
    "        elif lag_sample < 0:\n",
    "            aligned = np.pad(tr.data[:lag_sample], (-lag_sample, 0), \n",
    "                           mode='constant', constant_values=0)\n",
    "        else:\n",
    "            aligned = tr.data.copy()\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize:\n",
    "            aligned = aligned / np.max(np.abs(aligned))\n",
    "        \n",
    "        # Store aligned trace\n",
    "        tr_aligned = tr.copy()\n",
    "        tr_aligned.data = aligned\n",
    "        aligned_stream.append(tr_aligned)\n",
    "        aligned_data.append(aligned)\n",
    "    \n",
    "    if low_cc_stations:\n",
    "        print(f\"\\n  WARNING: {len(low_cc_stations)} stations with CC < {min_cc_threshold}: {low_cc_stations}\")\n",
    "    \n",
    "    # Stack aligned traces\n",
    "    stacked_data = np.mean(aligned_data, axis=0)\n",
    "    \n",
    "    # Create reference trace (copy stats from pilot)\n",
    "    reference_trace = pilot_trace.copy()\n",
    "    reference_trace.data = stacked_data\n",
    "    reference_trace.stats.station = 'STACK'\n",
    "    \n",
    "    print(f\"\\n  Reference waveform created from {len(aligned_data)} traces\")\n",
    "    print(f\"  Mean CC: {np.mean(list(alignment_quality.values())):.3f}\")\n",
    "    \n",
    "    return reference_trace, aligned_stream, snr_dict, alignment_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_trace, aligned_stream, snr_dict, alignment_quality = create_reference_waveform(\n",
    "    event_stream, \n",
    "    pick_times,\n",
    "    snr_window_signal=0.02, \n",
    "    snr_window_noise=0.02,\n",
    "    cc_window_length=0.03, \n",
    "    max_lag_sec=0.01,\n",
    "    min_cc_threshold=0.7, \n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b31129",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_event_id = df_picks['eventid'].iloc[9420]  \n",
    "second_event_id = df_picks['eventid'].iloc[10921]  # Note: should be 'eventid' not 'origin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed655a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_waveform_for_event(stream, df_picks, event_id, \n",
    "                                      pre_pick=0.02, post_pick=0.06,\n",
    "                                      cc_window_length=0.03, max_lag_sec=0.01):\n",
    "    \"\"\"\n",
    "    Extract traces and create reference waveform for a single event.\n",
    "    Convenience wrapper around extract_event_traces and create_reference_waveform.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stream : obspy.Stream\n",
    "        Full stream containing all data\n",
    "    df_picks : pandas.DataFrame\n",
    "        DataFrame with picks\n",
    "    event_id : str or int\n",
    "        Event identifier\n",
    "    pre_pick : float\n",
    "        Time (s) before pick to window\n",
    "    post_pick : float\n",
    "        Time (s) after pick to window\n",
    "    cc_window_length : float\n",
    "        CC window length for alignment (s)\n",
    "    max_lag_sec : float\n",
    "        Maximum lag for alignment (s)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    reference_trace : obspy.Trace\n",
    "        Reference waveform for this event\n",
    "    event_info : dict\n",
    "        Dictionary containing event_stream, aligned_stream, snr_dict, \n",
    "        alignment_quality for QC\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Event: {event_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract windowed traces\n",
    "    event_stream, pick_times = extract_event_traces(\n",
    "        stream, df_picks, event_id, \n",
    "        pre_pick=pre_pick, \n",
    "        post_pick=post_pick\n",
    "    )\n",
    "    \n",
    "    # Create reference waveform\n",
    "    reference_trace, aligned_stream, snr_dict, alignment_quality = create_reference_waveform(\n",
    "        event_stream, pick_times,\n",
    "        cc_window_length=cc_window_length,\n",
    "        max_lag_sec=max_lag_sec\n",
    "    )\n",
    "    \n",
    "    # Package info for return\n",
    "    event_info = {\n",
    "        'event_stream': event_stream,\n",
    "        'aligned_stream': aligned_stream,\n",
    "        'snr_dict': snr_dict,\n",
    "        'alignment_quality': alignment_quality,\n",
    "        'pick_times': pick_times,\n",
    "        'mean_snr': np.mean(list(snr_dict.values())),\n",
    "        'mean_cc': np.mean(list(alignment_quality.values()))\n",
    "    }\n",
    "    \n",
    "    return reference_trace, event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your event IDs\n",
    "first_event_id = df_picks['eventid'].iloc[9420]  \n",
    "second_event_id = df_picks['eventid'].iloc[10921]\n",
    "\n",
    "# Create reference waveforms for both events\n",
    "ref1, info1 = get_reference_waveform_for_event(comb_stZ, df_picks, first_event_id)\n",
    "ref2, info2 = get_reference_waveform_for_event(comb_stZ, df_picks, second_event_id)\n",
    "\n",
    "# Quick comparison plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(ref1.times(), ref1.data, 'b', label=f'Event {first_event_id}', linewidth=2)\n",
    "ax.plot(ref2.times(), ref2.data, 'r', label=f'Event {second_event_id}', linewidth=2)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Normalized Amplitude')\n",
    "ax.legend()\n",
    "ax.set_title('Reference Waveforms Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_waveforms_stretching(ref_trace1, ref_trace2, event_id1, event_id2,\n",
    "                                 stretch_max=0.10, stretch_step=0.01,\n",
    "                                 pulse_window_start=0.0, pulse_window_end=0.03):\n",
    "    \"\"\"\n",
    "    Compare two reference waveforms using trace stretching method.\n",
    "    Stretches ref_trace2 (comparison event) to match ref_trace1 (reference event).\n",
    "    Only stretches a specified window capturing the first P-wave pulse.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ref_trace1 : obspy.Trace\n",
    "        Reference waveform for REFERENCE event (typically early event)\n",
    "    ref_trace2 : obspy.Trace\n",
    "        Reference waveform for COMPARISON event (typically later event)\n",
    "    event_id1 : str or int\n",
    "        Identifier for reference event\n",
    "    event_id2 : str or int\n",
    "        Identifier for comparison event\n",
    "    stretch_max : float\n",
    "        Maximum stretch factor to test (e.g., 0.10 = ±10%)\n",
    "    stretch_step : float\n",
    "        Step size for stretch factor (e.g., 0.01 = 1% increments)\n",
    "    pulse_window_start : float\n",
    "        Start time (s) of pulse window to analyze (relative to trace start)\n",
    "    pulse_window_end : float\n",
    "        End time (s) of pulse window to analyze (relative to trace start)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing:\n",
    "        - 'epsilon': optimal stretch factor\n",
    "            ε > 0: comparison event pulse is LONGER (lower fc)\n",
    "            ε < 0: comparison event pulse is SHORTER (higher fc)\n",
    "        - 'cc': correlation coefficient at optimal stretch\n",
    "        - 'eps_array': array of tested stretch values\n",
    "        - 'cc_array': array of correlation coefficients\n",
    "        - 'pulse_window': tuple of (start, end) times used\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Comparing Events (Trace Stretching)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Reference event: {event_id1}\")\n",
    "    print(f\"  Comparison event: {event_id2}\")\n",
    "    print(f\"  Method: Stretch Event {event_id2} to match Event {event_id1}\")\n",
    "    \n",
    "    dt = ref_trace1.stats.delta\n",
    "    sampling_rate = ref_trace1.stats.sampling_rate\n",
    "    \n",
    "    # Convert time window to sample indices\n",
    "    start_sample = int(pulse_window_start * sampling_rate)\n",
    "    end_sample = int(pulse_window_end * sampling_rate)\n",
    "    \n",
    "    # Extract only the pulse window from both traces\n",
    "    u1_full = ref_trace1.data\n",
    "    u2_full = ref_trace2.data\n",
    "    \n",
    "    # Make sure we don't exceed trace length\n",
    "    end_sample = min(end_sample, len(u1_full), len(u2_full))\n",
    "    \n",
    "    u1 = u1_full[start_sample:end_sample]  # Reference (not stretched)\n",
    "    u2 = u2_full[start_sample:end_sample]  # This will be stretched\n",
    "    \n",
    "    pulse_duration = len(u1) * dt\n",
    "    \n",
    "    print(f\"  Full trace length: {len(u1_full)} samples ({len(u1_full) * dt:.3f} s)\")\n",
    "    print(f\"  Pulse window: {pulse_window_start:.3f} - {pulse_window_end:.3f} s\")\n",
    "    print(f\"  Pulse samples: {len(u1)} samples ({pulse_duration:.3f} s)\")\n",
    "    print(f\"  Sampling rate: {sampling_rate} Hz\")\n",
    "    print(f\"  Stretch range: ±{stretch_max*100:.0f}%\")\n",
    "    print(f\"  Stretch step: {stretch_step*100:.1f}%\")\n",
    "    \n",
    "    # Create array of stretch factors to test\n",
    "    eps_array = np.arange(-stretch_max, stretch_max + stretch_step, stretch_step)\n",
    "    cc_array = np.zeros(len(eps_array))\n",
    "    \n",
    "    # Test each stretch factor - NOW STRETCHING u2 (comparison) to match u1 (reference)\n",
    "    print(f\"\\n  Testing {len(eps_array)} stretch values...\")\n",
    "    for i, eps in enumerate(eps_array):\n",
    "        # Stretch the time axis of COMPARISON trace (u2)\n",
    "        t_stretched = np.arange(len(u2)) * (1 + eps)\n",
    "        \n",
    "        # Interpolate u2 onto stretched time axis\n",
    "        u2_stretched = pchip_interpolate(np.arange(len(u2)), u2, t_stretched, axis=0)\n",
    "        \n",
    "        # Ensure same length after interpolation\n",
    "        valid_len = min(len(u2_stretched), len(u1))\n",
    "        \n",
    "        # Calculate correlation coefficient between reference (u1) and stretched comparison (u2)\n",
    "        cc_array[i] = np.corrcoef(u1[:valid_len], u2_stretched[:valid_len])[0, 1]\n",
    "    \n",
    "    # Find optimal stretch factor (maximum correlation)\n",
    "    best_idx = np.argmax(cc_array)\n",
    "    best_eps = eps_array[best_idx]\n",
    "    best_cc = cc_array[best_idx]\n",
    "    \n",
    "    print(f\"\\n  Optimal stretch: {best_eps*100:.2f}% (ε = {best_eps:.4f})\")\n",
    "    print(f\"  Max correlation: {best_cc:.4f}\")\n",
    "    \n",
    "    # Interpretation (now correct!)\n",
    "    if abs(best_eps) < 0.01:\n",
    "        interp = \"Events have very similar pulse durations\"\n",
    "    elif best_eps > 0:\n",
    "        interp = f\"Comparison event pulse is ~{abs(best_eps)*100:.1f}% LONGER (lower fc)\"\n",
    "    else:\n",
    "        interp = f\"Comparison event pulse is ~{abs(best_eps)*100:.1f}% SHORTER (higher fc)\"\n",
    "    print(f\"  Interpretation: {interp}\")\n",
    "    \n",
    "    results = {\n",
    "        'epsilon': best_eps,\n",
    "        'cc': best_cc,\n",
    "        'eps_array': eps_array,\n",
    "        'cc_array': cc_array,\n",
    "        'event_id1': event_id1,\n",
    "        'event_id2': event_id2,\n",
    "        'pulse_window': (pulse_window_start, pulse_window_end),\n",
    "        'u1_pulse': u1,  # Reference pulse\n",
    "        'u2_pulse': u2   # Comparison pulse\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_stretching_results(ref_trace1, ref_trace2, results):\n",
    "    \"\"\"\n",
    "    Plot trace stretching comparison results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ref_trace1, ref_trace2 : obspy.Trace\n",
    "        Reference waveforms\n",
    "    results : dict\n",
    "        Results from compare_waveforms_stretching()\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Top panel: Full waveforms with pulse window highlighted\n",
    "    ax = axes[0]\n",
    "    ax.plot(ref_trace1.times(), ref_trace1.data, 'b', \n",
    "            label=f\"Event {results['event_id1']}\", linewidth=2)\n",
    "    ax.plot(ref_trace2.times(), ref_trace2.data, 'r', \n",
    "            label=f\"Event {results['event_id2']}\", linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Highlight the pulse window used for stretching\n",
    "    ax.axvspan(results['pulse_window'][0], results['pulse_window'][1], \n",
    "               alpha=0.2, color='green', label='Pulse window')\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Normalized Amplitude')\n",
    "    ax.legend()\n",
    "    ax.set_title('Full Reference Waveforms (pulse window highlighted)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Middle panel: Just the pulse windows\n",
    "    ax = axes[1]\n",
    "    dt = ref_trace1.stats.delta\n",
    "    pulse_times = np.arange(len(results['u1_pulse'])) * dt + results['pulse_window'][0]\n",
    "    \n",
    "    ax.plot(pulse_times, results['u1_pulse'], 'b', \n",
    "            label=f\"Event {results['event_id1']}\", linewidth=2)\n",
    "    ax.plot(pulse_times, results['u2_pulse'], 'r', \n",
    "            label=f\"Event {results['event_id2']}\", linewidth=2, alpha=0.7)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Normalized Amplitude')\n",
    "    ax.legend()\n",
    "    ax.set_title('P-Wave Pulse Windows (used for stretching)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom panel: Correlation vs stretch\n",
    "    ax = axes[2]\n",
    "    ax.plot(results['eps_array'] * 100, results['cc_array'], 'k-', linewidth=2)\n",
    "    ax.axvline(results['epsilon'] * 100, color='r', linestyle='--', \n",
    "               label=f\"Best ε = {results['epsilon']*100:.2f}%\", linewidth=2)\n",
    "    ax.axhline(results['cc'], color='r', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Stretch Factor (%)')\n",
    "    ax.set_ylabel('Correlation Coefficient')\n",
    "    ax.set_title(f\"Trace Stretching: ε = {results['epsilon']*100:.2f}%, CC = {results['cc']:.4f}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([results['eps_array'].min()*100, results['eps_array'].max()*100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3487386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corner_frequency(trace, method='brune', f_min=10, f_max=100, \n",
    "                                taper_percent=0.05, zero_pad_factor=4):\n",
    "    \"\"\"\n",
    "    Calculate corner frequency from a trace's amplitude spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trace : obspy.Trace\n",
    "        Input trace (preferably full reference waveform, not just pulse)\n",
    "    method : str\n",
    "        Method for corner frequency estimation ('brune' or 'peak')\n",
    "    f_min : float\n",
    "        Minimum frequency (Hz) for fitting\n",
    "    f_max : float\n",
    "        Maximum frequency (Hz) for fitting\n",
    "    taper_percent : float\n",
    "        Taper percentage for edges (0-1)\n",
    "    zero_pad_factor : int\n",
    "        Zero-padding factor to increase frequency resolution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fc : float\n",
    "        Corner frequency (Hz)\n",
    "    fit_info : dict\n",
    "        Dictionary with spectrum, frequencies, and fit details\n",
    "    \"\"\"\n",
    "    \n",
    "    # Taper the trace to reduce edge effects\n",
    "    tr = trace.copy()\n",
    "    tr.taper(max_percentage=taper_percent, type='cosine')\n",
    "    \n",
    "    # Get trace parameters\n",
    "    npts = tr.stats.npts\n",
    "    dt = tr.stats.delta\n",
    "    sampling_rate = tr.stats.sampling_rate\n",
    "    \n",
    "    print(f\"    Trace info:\")\n",
    "    print(f\"      Length: {npts} samples ({npts*dt:.4f} s)\")\n",
    "    print(f\"      Sampling rate: {sampling_rate} Hz\")\n",
    "    \n",
    "    # Zero-pad to improve frequency resolution\n",
    "    npts_padded = npts * zero_pad_factor\n",
    "    data_padded = np.pad(tr.data, (0, npts_padded - npts), mode='constant')\n",
    "    \n",
    "    # Calculate FFT\n",
    "    fft = np.fft.rfft(data_padded)\n",
    "    freqs = np.fft.rfftfreq(npts_padded, dt)\n",
    "    amp_spectrum = np.abs(fft)\n",
    "    \n",
    "    freq_resolution = 1.0 / (npts_padded * dt)\n",
    "    print(f\"      Frequency resolution: {freq_resolution:.2f} Hz\")\n",
    "    print(f\"      Number of frequency points: {len(freqs)}\")\n",
    "    \n",
    "    # Frequency band for analysis (within your bandpass filter)\n",
    "    freq_mask = (freqs >= f_min) & (freqs <= f_max)\n",
    "    freqs_fit = freqs[freq_mask]\n",
    "    amp_fit = amp_spectrum[freq_mask]\n",
    "    \n",
    "    print(f\"      Points in fit band ({f_min}-{f_max} Hz): {len(freqs_fit)}\")\n",
    "    \n",
    "    if len(freqs_fit) < 10:\n",
    "        print(f\"      WARNING: Very few frequency points! Consider longer trace.\")\n",
    "    \n",
    "    if method == 'brune':\n",
    "        # Fit Brune (1970) spectrum: A(f) = Ω₀ / (1 + (f/fc)²)\n",
    "        \n",
    "        # Smooth spectrum for more stable fit\n",
    "        from scipy.ndimage import gaussian_filter1d\n",
    "        if len(amp_fit) > 5:\n",
    "            sigma = max(1, len(amp_fit) // 20)  # Adaptive smoothing\n",
    "            amp_smooth = gaussian_filter1d(amp_fit, sigma=sigma)\n",
    "        else:\n",
    "            amp_smooth = amp_fit\n",
    "        \n",
    "        # Find low-frequency plateau (Ω₀) - use median of lowest 20% of frequencies\n",
    "        n_low = max(3, len(amp_smooth) // 5)\n",
    "        omega_0 = np.median(amp_smooth[:n_low])\n",
    "        \n",
    "        # Find frequency where amplitude drops to omega_0 / sqrt(2)\n",
    "        target_amp = omega_0 / np.sqrt(2)\n",
    "        \n",
    "        # Find crossing point\n",
    "        idx = np.argmin(np.abs(amp_smooth - target_amp))\n",
    "        fc = freqs_fit[idx]\n",
    "        \n",
    "        # Make sure we didn't hit the boundary\n",
    "        if fc >= f_max * 0.95:\n",
    "            print(f\"      WARNING: Corner frequency at upper bound! May be >f_max\")\n",
    "        if fc <= f_min * 1.05:\n",
    "            print(f\"      WARNING: Corner frequency at lower bound! May be <f_min\")\n",
    "        \n",
    "        print(f\"    Brune model fit:\")\n",
    "        print(f\"      Ω₀ (low-freq plateau): {omega_0:.2e}\")\n",
    "        print(f\"      Corner frequency: {fc:.1f} Hz\")\n",
    "        \n",
    "        fit_info = {\n",
    "            'freqs': freqs,\n",
    "            'amp_spectrum': amp_spectrum,\n",
    "            'freqs_fit': freqs_fit,\n",
    "            'amp_fit': amp_fit,\n",
    "            'amp_smooth': amp_smooth,\n",
    "            'omega_0': omega_0,\n",
    "            'method': 'brune',\n",
    "            'freq_resolution': freq_resolution\n",
    "        }\n",
    "        \n",
    "    elif method == 'peak':\n",
    "        # Alternative: Use peak frequency as proxy\n",
    "        idx_peak = np.argmax(amp_fit)\n",
    "        fc = freqs_fit[idx_peak]\n",
    "        \n",
    "        print(f\"    Peak frequency method:\")\n",
    "        print(f\"      Peak frequency: {fc:.1f} Hz\")\n",
    "        \n",
    "        fit_info = {\n",
    "            'freqs': freqs,\n",
    "            'amp_spectrum': amp_spectrum,\n",
    "            'freqs_fit': freqs_fit,\n",
    "            'amp_fit': amp_fit,\n",
    "            'method': 'peak',\n",
    "            'freq_resolution': freq_resolution\n",
    "        }\n",
    "    \n",
    "    return fc, fit_info\n",
    "\n",
    "def plot_spectrum_with_fc(trace, fc, fit_info, event_id):\n",
    "    \"\"\"\n",
    "    Plot amplitude spectrum with corner frequency marked.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trace : obspy.Trace\n",
    "        Input trace\n",
    "    fc : float\n",
    "        Corner frequency (Hz)\n",
    "    fit_info : dict\n",
    "        Fit information from calculate_corner_frequency\n",
    "    event_id : str/int\n",
    "        Event identifier for labeling\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Plot full spectrum\n",
    "    ax.loglog(fit_info['freqs'], fit_info['amp_spectrum'], 'gray', \n",
    "              alpha=0.3, linewidth=0.5, label='Full spectrum')\n",
    "    \n",
    "    # Plot fitting region\n",
    "    ax.loglog(fit_info['freqs_fit'], fit_info['amp_fit'], 'k', \n",
    "              linewidth=1.5, label='Fit region')\n",
    "    \n",
    "    if fit_info['method'] == 'brune':\n",
    "        # Plot smoothed spectrum\n",
    "        ax.loglog(fit_info['freqs_fit'], fit_info['amp_smooth'], 'b', \n",
    "                  linewidth=2, label='Smoothed')\n",
    "        \n",
    "        # Mark Ω₀ level\n",
    "        ax.axhline(fit_info['omega_0'], color='green', linestyle='--', \n",
    "                   alpha=0.5, label=f\"Ω₀ = {fit_info['omega_0']:.2e}\")\n",
    "        \n",
    "        # Mark Ω₀/√2 level\n",
    "        ax.axhline(fit_info['omega_0']/np.sqrt(2), color='orange', \n",
    "                   linestyle=':', alpha=0.5, label=f\"Ω₀/√2\")\n",
    "    \n",
    "    # Mark corner frequency\n",
    "    ax.axvline(fc, color='red', linestyle='--', linewidth=2,\n",
    "               label=f\"fc = {fc:.1f} Hz\")\n",
    "    \n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Amplitude Spectrum')\n",
    "    ax.set_title(f'Amplitude Spectrum - Event {event_id}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, which='both')\n",
    "    ax.set_xlim([5, 150])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_waveforms_stretching(\n",
    "    ref1, ref2,  # ref1 is reference, ref2 is stretched\n",
    "    first_event_id, second_event_id,\n",
    "    stretch_max=0.10,\n",
    "    stretch_step=0.01,\n",
    "    pulse_window_start=0.01,\n",
    "    pulse_window_end=0.04\n",
    ")\n",
    "\n",
    "# Now the corner frequency ratio should match!\n",
    "fc_ratio_spectral = fc2 / fc1\n",
    "fc_ratio_stretching = 1 / (1 + results['epsilon'])\n",
    "\n",
    "print(f\"\\nCorner frequency ratio (from spectra): {fc_ratio_spectral:.3f}\")\n",
    "print(f\"Corner frequency ratio (from stretching): {fc_ratio_stretching:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Get event IDs\n",
    "first_event_id = df_picks['eventid'].iloc[9420]  \n",
    "second_event_id = df_picks['eventid'].iloc[10921]\n",
    "\n",
    "# STEP 2: Create reference waveforms\n",
    "ref1, info1 = get_reference_waveform_for_event(comb_stZ, df_picks, first_event_id)\n",
    "ref2, info2 = get_reference_waveform_for_event(comb_stZ, df_picks, second_event_id)\n",
    "\n",
    "# STEP 3: Run stretching analysis (use UPDATED function I just gave you)\n",
    "results = compare_waveforms_stretching(\n",
    "    ref1, ref2,\n",
    "    first_event_id, second_event_id,\n",
    "    stretch_max=0.10,\n",
    "    stretch_step=0.01,\n",
    "    pulse_window_start=0.01,\n",
    "    pulse_window_end=0.048\n",
    ")\n",
    "\n",
    "# STEP 4: Plot stretching results\n",
    "plot_stretching_results(ref1, ref2, results)\n",
    "\n",
    "# STEP 5: Calculate corner frequencies from full reference traces\n",
    "fc1, fit1 = calculate_corner_frequency(ref1, method='brune', f_min=15, f_max=80)\n",
    "fc2, fit2 = calculate_corner_frequency(ref2, method='brune', f_min=15, f_max=80)\n",
    "\n",
    "# STEP 6: Compare the two methods\n",
    "fc_ratio_spectral = fc2 / fc1\n",
    "fc_ratio_stretching = 1 / (1 + results['epsilon'])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPARISON: SPECTRAL vs STRETCHING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Event 1 (reference): fc = {fc1:.1f} Hz\")\n",
    "print(f\"Event 2 (comparison): fc = {fc2:.1f} Hz\")\n",
    "print(f\"\\nCorner frequency ratio (from spectra): {fc_ratio_spectral:.3f}\")\n",
    "print(f\"Corner frequency ratio (from stretching): {fc_ratio_stretching:.3f}\")\n",
    "print(f\"Difference: {abs(fc_ratio_spectral - fc_ratio_stretching):.3f}\")\n",
    "print(f\"Percent difference: {abs(fc_ratio_spectral - fc_ratio_stretching)/fc_ratio_spectral * 100:.1f}%\")\n",
    "\n",
    "if abs(fc_ratio_spectral - fc_ratio_stretching) / fc_ratio_spectral < 0.15:\n",
    "    print(\"\\n✓ Good agreement! Stretching analysis is valid.\")\n",
    "else:\n",
    "    print(\"\\n✗ Poor agreement. Investigate further.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stress_drop_ratio(epsilon, fc_ratio=None):\n",
    "    \"\"\"\n",
    "    Calculate relative stress drop from stretch factor or corner frequency ratio.\n",
    "    \n",
    "    Based on Brune (1970) stress drop scaling:\n",
    "        Δσ ∝ fc³\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epsilon : float\n",
    "        Stretch factor from trace stretching\n",
    "        ε > 0: comparison event has longer pulse (lower fc, lower stress drop)\n",
    "        ε < 0: comparison event has shorter pulse (higher fc, higher stress drop)\n",
    "    fc_ratio : float, optional\n",
    "        Corner frequency ratio (fc2/fc1) from spectral analysis\n",
    "        If provided, also calculates stress drop ratio from spectra\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing:\n",
    "        - 'stress_drop_ratio_stretch': Δσ2/Δσ1 from stretching\n",
    "        - 'stress_drop_ratio_spectral': Δσ2/Δσ1 from spectra (if fc_ratio provided)\n",
    "        - 'fc_ratio_stretch': fc2/fc1 from stretching\n",
    "        - 'fc_ratio_spectral': fc2/fc1 from spectra (if provided)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Corner frequency ratio from stretching\n",
    "    # fc2/fc1 = 1/(1+ε)\n",
    "    fc_ratio_stretch = 1.0 / (1.0 + epsilon)\n",
    "    \n",
    "    # Stress drop ratio from stretching\n",
    "    # Δσ2/Δσ1 = (fc2/fc1)³\n",
    "    stress_ratio_stretch = fc_ratio_stretch ** 3\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STRESS DROP ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nFrom Trace Stretching:\")\n",
    "    print(f\"  Stretch factor (ε): {epsilon:.4f} ({epsilon*100:.2f}%)\")\n",
    "    print(f\"  Corner frequency ratio (fc2/fc1): {fc_ratio_stretch:.3f}\")\n",
    "    print(f\"  Stress drop ratio (Δσ2/Δσ1): {stress_ratio_stretch:.3f}\")\n",
    "    \n",
    "    if epsilon > 0:\n",
    "        print(f\"  → Comparison event has {abs(epsilon)*100:.1f}% LONGER pulse\")\n",
    "        print(f\"  → Comparison event has {(1-stress_ratio_stretch)*100:.1f}% LOWER stress drop\")\n",
    "    elif epsilon < 0:\n",
    "        print(f\"  → Comparison event has {abs(epsilon)*100:.1f}% SHORTER pulse\")\n",
    "        print(f\"  → Comparison event has {(stress_ratio_stretch-1)*100:.1f}% HIGHER stress drop\")\n",
    "    else:\n",
    "        print(f\"  → Events have similar stress drops\")\n",
    "    \n",
    "    results = {\n",
    "        'stress_drop_ratio_stretch': stress_ratio_stretch,\n",
    "        'fc_ratio_stretch': fc_ratio_stretch\n",
    "    }\n",
    "    \n",
    "    # If spectral corner frequencies provided, calculate from those too\n",
    "    if fc_ratio is not None:\n",
    "        stress_ratio_spectral = fc_ratio ** 3\n",
    "        \n",
    "        print(f\"\\nFrom Spectral Analysis:\")\n",
    "        print(f\"  Corner frequency ratio (fc2/fc1): {fc_ratio:.3f}\")\n",
    "        print(f\"  Stress drop ratio (Δσ2/Δσ1): {stress_ratio_spectral:.3f}\")\n",
    "        \n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  Stress drop ratio difference: {abs(stress_ratio_stretch - stress_ratio_spectral):.3f}\")\n",
    "        print(f\"  Percent difference: {abs(stress_ratio_stretch - stress_ratio_spectral)/stress_ratio_spectral*100:.1f}%\")\n",
    "        \n",
    "        results['stress_drop_ratio_spectral'] = stress_ratio_spectral\n",
    "        results['fc_ratio_spectral'] = fc_ratio\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6da313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running your comparison from before, add:\n",
    "\n",
    "# Calculate stress drop ratios\n",
    "stress_results = calculate_stress_drop_ratio(\n",
    "    results['epsilon'],\n",
    "    fc_ratio=fc2/fc1  # Optional: compare with spectral estimate\n",
    ")\n",
    "\n",
    "# Store everything together\n",
    "comparison_summary = {\n",
    "    'event_id1': first_event_id,\n",
    "    'event_id2': second_event_id,\n",
    "    'epsilon': results['epsilon'],\n",
    "    'cc': results['cc'],\n",
    "    'fc1': fc1,\n",
    "    'fc2': fc2,\n",
    "    'fc_ratio_spectral': fc2/fc1,\n",
    "    'fc_ratio_stretch': stress_results['fc_ratio_stretch'],\n",
    "    'stress_ratio_spectral': stress_results['stress_drop_ratio_spectral'],\n",
    "    'stress_ratio_stretch': stress_results['stress_drop_ratio_stretch']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for key, val in comparison_summary.items():\n",
    "    if isinstance(val, float):\n",
    "        print(f\"  {key}: {val:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_peak_amplitude(event_stream, pick_times, window_length=0.03):\n",
    "    \"\"\"\n",
    "    Calculate peak amplitude from raw (unnormalized) traces around P-wave arrival.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_stream : obspy.Stream\n",
    "        Raw windowed traces for an event (before stacking/normalization)\n",
    "    pick_times : dict\n",
    "        Dictionary of pick times for each station\n",
    "    window_length : float\n",
    "        Time window (s) after pick to search for peak amplitude\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    peak_amplitude : float\n",
    "        Maximum absolute amplitude across all stations (m/s for velocity)\n",
    "    median_amplitude : float\n",
    "        Median peak amplitude across stations\n",
    "    amplitude_dict : dict\n",
    "        Peak amplitude for each station\n",
    "    \"\"\"\n",
    "    \n",
    "    amplitude_dict = {}\n",
    "    \n",
    "    for tr in event_stream:\n",
    "        station = tr.stats.station\n",
    "        \n",
    "        if station not in pick_times:\n",
    "            continue\n",
    "        \n",
    "        # Get sample index of pick\n",
    "        pick_time = pick_times[station]\n",
    "        pick_sample = int((pick_time - tr.stats.starttime) * tr.stats.sampling_rate)\n",
    "        \n",
    "        # Define window after pick\n",
    "        window_samples = int(window_length * tr.stats.sampling_rate)\n",
    "        end_sample = min(pick_sample + window_samples, tr.stats.npts)\n",
    "        \n",
    "        if pick_sample >= tr.stats.npts or pick_sample < 0:\n",
    "            continue\n",
    "        \n",
    "        # Get peak amplitude in window\n",
    "        signal_window = tr.data[pick_sample:end_sample]\n",
    "        peak_amp = np.max(np.abs(signal_window))\n",
    "        amplitude_dict[station] = peak_amp\n",
    "    \n",
    "    if len(amplitude_dict) == 0:\n",
    "        return np.nan, np.nan, {}\n",
    "    \n",
    "    amplitudes = list(amplitude_dict.values())\n",
    "    peak_amplitude = np.max(amplitudes)\n",
    "    median_amplitude = np.median(amplitudes)\n",
    "    \n",
    "    return peak_amplitude, median_amplitude, amplitude_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_evolution_robust(stream, df_picks, reference_event_id, \n",
    "                                      comparison_event_ids,\n",
    "                                      pulse_window_start=0.01, pulse_window_end=0.04,\n",
    "                                      stretch_max=0.20, stretch_step=0.01,\n",
    "                                      calculate_spectral_fc=True,\n",
    "                                      min_stretch_cc=0.70,\n",
    "                                      min_alignment_cc=0.70,\n",
    "                                      min_snr=3.0):\n",
    "    \"\"\"\n",
    "    Robust temporal evolution analysis with quality control filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stream : obspy.Stream\n",
    "        Full data stream\n",
    "    df_picks : pandas.DataFrame\n",
    "        Picks dataframe\n",
    "    reference_event_id : str/int\n",
    "        Event ID for reference\n",
    "    comparison_event_ids : list\n",
    "        List of event IDs to analyze\n",
    "    pulse_window_start : float\n",
    "        Start of pulse window (s)\n",
    "    pulse_window_end : float\n",
    "        End of pulse window (s)\n",
    "    stretch_max : float\n",
    "        Maximum stretch factor\n",
    "    stretch_step : float\n",
    "        Stretch step size\n",
    "    calculate_spectral_fc : bool\n",
    "        If True, calculate corner frequency from spectrum for each event\n",
    "    min_stretch_cc : float\n",
    "        Minimum correlation coefficient for stretching analysis (0-1)\n",
    "        Events below this threshold are flagged/rejected\n",
    "    min_alignment_cc : float\n",
    "        Minimum mean correlation coefficient for trace stacking (0-1)\n",
    "    min_snr : float\n",
    "        Minimum mean SNR for event stacking\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame with all analyzed events\n",
    "    qc_stats : dict\n",
    "        Quality control statistics\n",
    "    reference_info : dict\n",
    "        Reference event information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ROBUST TEMPORAL EVOLUTION ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nQuality Control Thresholds:\")\n",
    "    print(f\"  Min stretch CC: {min_stretch_cc:.2f}\")\n",
    "    print(f\"  Min alignment CC: {min_alignment_cc:.2f}\")\n",
    "    print(f\"  Min SNR: {min_snr:.1f}\")\n",
    "    \n",
    "    # Get reference event info\n",
    "    ref_rows = df_picks[df_picks['eventid'] == reference_event_id]\n",
    "    ref_origin = UTCDateTime(ref_rows['origin'].iloc[0])\n",
    "    \n",
    "    print(f\"\\nReference Event: {reference_event_id}\")\n",
    "    print(f\"  Origin time: {ref_origin}\")\n",
    "    \n",
    "    # Create reference waveform\n",
    "    print(f\"\\nCreating reference waveform...\")\n",
    "    \n",
    "    # Get RAW event stream (before stacking) for amplitude\n",
    "    ref_event_stream, ref_pick_times = extract_event_traces(\n",
    "        stream, df_picks, reference_event_id,\n",
    "        pre_pick=0.02, post_pick=0.06\n",
    "    )\n",
    "    \n",
    "    # Get amplitudes from raw traces\n",
    "    ref_peak_amp, ref_median_amp, ref_amp_dict = get_event_peak_amplitude(\n",
    "        ref_event_stream, ref_pick_times, window_length=0.03\n",
    "    )\n",
    "    \n",
    "    # Create stacked reference for stretching\n",
    "    ref_trace, ref_info = get_reference_waveform_for_event(\n",
    "        stream, df_picks, reference_event_id,\n",
    "        pre_pick=0.02, post_pick=0.06\n",
    "    )\n",
    "    \n",
    "    # Calculate reference corner frequency from spectrum\n",
    "    ref_fc_spectral, ref_fit = calculate_corner_frequency(\n",
    "        ref_trace, method='brune', f_min=15, f_max=80\n",
    "    )\n",
    "    \n",
    "    print(f\"  Reference fc (spectral): {ref_fc_spectral:.1f} Hz\")\n",
    "    print(f\"  Reference peak amplitude: {ref_peak_amp:.2e} m/s\")\n",
    "    print(f\"  Reference median amplitude: {ref_median_amp:.2e} m/s\")\n",
    "    \n",
    "    # Storage\n",
    "    results_list = []\n",
    "    qc_rejected = []\n",
    "    \n",
    "    # QC counters\n",
    "    n_total = 0\n",
    "    n_low_stretch_cc = 0\n",
    "    n_low_alignment_cc = 0\n",
    "    n_low_snr = 0\n",
    "    n_accepted = 0\n",
    "    \n",
    "    # Add all events to list (including reference)\n",
    "    all_event_ids = [reference_event_id] + comparison_event_ids\n",
    "    \n",
    "    for i, event_id in enumerate(all_event_ids):\n",
    "        \n",
    "        # Get origin time\n",
    "        event_rows = df_picks[df_picks['eventid'] == event_id]\n",
    "        if len(event_rows) == 0:\n",
    "            print(f\"\\nEvent {event_id}: No picks found, skipping\")\n",
    "            continue\n",
    "            \n",
    "        event_origin = UTCDateTime(event_rows['origin'].iloc[0])\n",
    "        time_diff = event_origin - ref_origin\n",
    "        \n",
    "        is_reference = (event_id == reference_event_id)\n",
    "        n_total += 1\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        if is_reference:\n",
    "            print(f\"Reference Event: {event_id}\")\n",
    "        else:\n",
    "            print(f\"Event {i}/{len(comparison_event_ids)}: {event_id}\")\n",
    "            print(f\"  Time since reference: {time_diff/60:.1f} minutes\")\n",
    "        \n",
    "        try:\n",
    "            # Get RAW event stream for amplitude\n",
    "            event_stream, event_pick_times = extract_event_traces(\n",
    "                stream, df_picks, event_id,\n",
    "                pre_pick=0.02, post_pick=0.06\n",
    "            )\n",
    "            \n",
    "            # Get amplitudes from raw traces\n",
    "            peak_amp, median_amp, amp_dict = get_event_peak_amplitude(\n",
    "                event_stream, event_pick_times, window_length=0.03\n",
    "            )\n",
    "            \n",
    "            # Create stacked waveform for spectral analysis and stretching\n",
    "            event_trace, event_info = get_reference_waveform_for_event(\n",
    "                stream, df_picks, event_id,\n",
    "                pre_pick=0.02, post_pick=0.06\n",
    "            )\n",
    "            \n",
    "            # Calculate spectral corner frequency\n",
    "            if calculate_spectral_fc:\n",
    "                fc_spectral, fit_info = calculate_corner_frequency(\n",
    "                    event_trace, method='brune', f_min=15, f_max=80\n",
    "                )\n",
    "            else:\n",
    "                fc_spectral = np.nan\n",
    "            \n",
    "            # Stretching analysis (skip for reference event)\n",
    "            if not is_reference:\n",
    "                stretch_results = compare_waveforms_stretching(\n",
    "                    ref_trace, event_trace,\n",
    "                    reference_event_id, event_id,\n",
    "                    stretch_max=stretch_max,\n",
    "                    stretch_step=stretch_step,\n",
    "                    pulse_window_start=pulse_window_start,\n",
    "                    pulse_window_end=pulse_window_end\n",
    "                )\n",
    "                epsilon = stretch_results['epsilon']\n",
    "                stretch_cc = stretch_results['cc']\n",
    "                fc_stretch = ref_fc_spectral / (1.0 + epsilon)\n",
    "            else:\n",
    "                epsilon = 0.0\n",
    "                stretch_cc = 1.0\n",
    "                fc_stretch = ref_fc_spectral\n",
    "            \n",
    "            # === QUALITY CONTROL CHECKS ===\n",
    "            qc_pass = True\n",
    "            qc_reasons = []\n",
    "            \n",
    "            # Check 1: Stretch correlation coefficient\n",
    "            if stretch_cc < min_stretch_cc and not is_reference:\n",
    "                qc_pass = False\n",
    "                qc_reasons.append(f\"Low stretch CC ({stretch_cc:.3f} < {min_stretch_cc})\")\n",
    "                n_low_stretch_cc += 1\n",
    "            \n",
    "            # Check 2: Stacking alignment quality\n",
    "            if event_info['mean_cc'] < min_alignment_cc:\n",
    "                qc_pass = False\n",
    "                qc_reasons.append(f\"Low alignment CC ({event_info['mean_cc']:.3f} < {min_alignment_cc})\")\n",
    "                n_low_alignment_cc += 1\n",
    "            \n",
    "            # Check 3: Mean SNR\n",
    "            if event_info['mean_snr'] < min_snr:\n",
    "                qc_pass = False\n",
    "                qc_reasons.append(f\"Low SNR ({event_info['mean_snr']:.1f} < {min_snr})\")\n",
    "                n_low_snr += 1\n",
    "            \n",
    "            # Print QC status\n",
    "            if not qc_pass:\n",
    "                print(f\"  ⚠ QC FAILED: {', '.join(qc_reasons)}\")\n",
    "            else:\n",
    "                print(f\"  ✓ QC PASSED\")\n",
    "                n_accepted += 1\n",
    "            \n",
    "            # Store results (include all, mark QC status)\n",
    "            results_list.append({\n",
    "                'event_id': event_id,\n",
    "                'origin_time': event_origin,\n",
    "                'time_since_ref_sec': time_diff,\n",
    "                'time_since_ref_min': time_diff / 60.0,\n",
    "                'is_reference': is_reference,\n",
    "                'qc_pass': qc_pass,\n",
    "                'qc_reasons': '; '.join(qc_reasons) if not qc_pass else '',\n",
    "                'fc_spectral': fc_spectral,\n",
    "                'fc_stretch': fc_stretch,\n",
    "                'epsilon': epsilon,\n",
    "                'stretch_cc': stretch_cc,\n",
    "                'peak_amplitude': peak_amp,\n",
    "                'median_amplitude': median_amp,\n",
    "                'mean_snr': event_info['mean_snr'],\n",
    "                'mean_alignment_cc': event_info['mean_cc'],\n",
    "                'n_stations': len(event_info['snr_dict'])\n",
    "            })\n",
    "            \n",
    "            print(f\"  fc (spectral): {fc_spectral:.1f} Hz\")\n",
    "            print(f\"  Peak amplitude: {peak_amp:.2e} m/s\")\n",
    "            print(f\"  ε: {epsilon:.4f}, stretch CC: {stretch_cc:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Calculate stress drop proxy\n",
    "    if len(results_df) > 0:\n",
    "        results_df['stress_drop_proxy'] = (results_df['peak_amplitude']**2) * results_df['fc_spectral']\n",
    "    \n",
    "    # QC Statistics\n",
    "    qc_stats = {\n",
    "        'n_total': n_total,\n",
    "        'n_accepted': n_accepted,\n",
    "        'n_rejected': n_total - n_accepted,\n",
    "        'n_low_stretch_cc': n_low_stretch_cc,\n",
    "        'n_low_alignment_cc': n_low_alignment_cc,\n",
    "        'n_low_snr': n_low_snr,\n",
    "        'acceptance_rate': n_accepted / n_total if n_total > 0 else 0\n",
    "    }\n",
    "    \n",
    "    reference_info = {\n",
    "        'event_id': reference_event_id,\n",
    "        'origin_time': ref_origin,\n",
    "        'fc_spectral': ref_fc_spectral,\n",
    "        'peak_amplitude': ref_peak_amp,\n",
    "        'median_amplitude': ref_median_amp,\n",
    "        'info': ref_info\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUALITY CONTROL SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total events analyzed: {qc_stats['n_total']}\")\n",
    "    print(f\"Passed QC: {qc_stats['n_accepted']} ({qc_stats['acceptance_rate']*100:.1f}%)\")\n",
    "    print(f\"Failed QC: {qc_stats['n_rejected']}\")\n",
    "    print(f\"  Low stretch CC: {qc_stats['n_low_stretch_cc']}\")\n",
    "    print(f\"  Low alignment CC: {qc_stats['n_low_alignment_cc']}\")\n",
    "    print(f\"  Low SNR: {qc_stats['n_low_snr']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results_df, qc_stats, reference_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_events(df_picks, sort_by_time=True):\n",
    "    \"\"\"\n",
    "    Get list of unique event IDs from picks dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_picks : pandas.DataFrame\n",
    "        Picks dataframe with 'eventid' and 'origin' columns\n",
    "    sort_by_time : bool\n",
    "        If True, sort events chronologically\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    event_ids : list\n",
    "        List of unique event IDs in chronological order\n",
    "    \"\"\"\n",
    "    \n",
    "    if sort_by_time:\n",
    "        # Get unique events and their origin times\n",
    "        unique_events = df_picks.groupby('eventid')['origin'].first().sort_values()\n",
    "        event_ids = unique_events.index.tolist()\n",
    "    else:\n",
    "        event_ids = df_picks['eventid'].unique().tolist()\n",
    "    \n",
    "    print(f\"Total unique events: {len(event_ids)}\")\n",
    "    \n",
    "    return event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temporal_evolution_robust(results_df, reference_info, show_rejected=False):\n",
    "    \"\"\"\n",
    "    Plot temporal evolution with absolute values, not ratios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        Results with QC information\n",
    "    reference_info : dict\n",
    "        Reference event info\n",
    "    show_rejected : bool\n",
    "        If True, show rejected events as faded points\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Separate QC pass/fail\n",
    "    qc_pass_mask = (results_df['is_reference'] == False) & (results_df['qc_pass'] == True)\n",
    "    qc_fail_mask = (results_df['is_reference'] == False) & (results_df['qc_pass'] == False)\n",
    "    \n",
    "    time_pass = results_df.loc[qc_pass_mask, 'time_since_ref_min'].values\n",
    "    time_fail = results_df.loc[qc_fail_mask, 'time_since_ref_min'].values\n",
    "    \n",
    "    ref_fc = reference_info['fc_spectral']\n",
    "    ref_amp = reference_info['peak_amplitude']\n",
    "    \n",
    "    # Panel 1: Corner Frequency (spectral)\n",
    "    ax = axes[0]\n",
    "    ax.scatter(time_pass, results_df.loc[qc_pass_mask, 'fc_spectral'].values, \n",
    "               c='blue', s=50, alpha=0.7, edgecolors='k', linewidth=0.5, label='QC Pass')\n",
    "    if show_rejected and len(time_fail) > 0:\n",
    "        ax.scatter(time_fail, results_df.loc[qc_fail_mask, 'fc_spectral'].values,\n",
    "                   c='lightgray', s=30, alpha=0.3, edgecolors='k', linewidth=0.5, label='QC Fail')\n",
    "    ax.axhline(ref_fc, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Reference fc = {ref_fc:.1f} Hz')\n",
    "    ax.set_ylabel('Corner Frequency (Hz)')\n",
    "    ax.set_title(f'Temporal Evolution (Reference: Event {reference_info[\"event_id\"]})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 2: Peak Amplitude\n",
    "    ax = axes[1]\n",
    "    ax.scatter(time_pass, results_df.loc[qc_pass_mask, 'peak_amplitude'].values, \n",
    "               c='green', s=50, alpha=0.7, edgecolors='k', linewidth=0.5, label='QC Pass')\n",
    "    if show_rejected and len(time_fail) > 0:\n",
    "        ax.scatter(time_fail, results_df.loc[qc_fail_mask, 'peak_amplitude'].values,\n",
    "                   c='lightgray', s=30, alpha=0.3, edgecolors='k', linewidth=0.5, label='QC Fail')\n",
    "    ax.axhline(ref_amp, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Reference amp = {ref_amp:.2e} m/s')\n",
    "    ax.set_ylabel('Peak Amplitude (m/s)')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Panel 3: Stress Drop Proxy\n",
    "    ax = axes[2]\n",
    "    ax.scatter(time_pass, results_df.loc[qc_pass_mask, 'stress_drop_proxy'].values,\n",
    "               c='red', s=50, alpha=0.7, edgecolors='k', linewidth=0.5, label='QC Pass')\n",
    "    if show_rejected and len(time_fail) > 0:\n",
    "        ax.scatter(time_fail, results_df.loc[qc_fail_mask, 'stress_drop_proxy'].values,\n",
    "                   c='lightgray', s=30, alpha=0.3, edgecolors='k', linewidth=0.5, label='QC Fail')\n",
    "    ref_stress = (ref_amp**2) * ref_fc\n",
    "    ax.axhline(ref_stress, color='red', linestyle='--', linewidth=2,\n",
    "               label='Reference')\n",
    "    ax.set_ylabel('Stress Drop Proxy\\n(amp² × fc)')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Panel 4: Stretch factor (for QC)\n",
    "    ax = axes[3]\n",
    "    ax.scatter(time_pass, results_df.loc[qc_pass_mask, 'epsilon'].values * 100,\n",
    "               c='purple', s=50, alpha=0.7, edgecolors='k', linewidth=0.5, label='QC Pass')\n",
    "    if show_rejected and len(time_fail) > 0:\n",
    "        ax.scatter(time_fail, results_df.loc[qc_fail_mask, 'epsilon'].values * 100,\n",
    "                   c='lightgray', s=30, alpha=0.3, edgecolors='k', linewidth=0.5, label='QC Fail')\n",
    "    ax.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_ylabel('Stretch Factor ε (%)')\n",
    "    ax.set_xlabel('Time Since Reference Event (minutes)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique events in chronological order\n",
    "all_event_ids = get_unique_events(df_picks, sort_by_time=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c93415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select reference event (first one or early one)\n",
    "# reference_event_id = all_event_ids[0]  # Or pick a specific one\n",
    "# ref_ind = np.where(first_event_id == all_event_ids)[0][0]\n",
    "ref_ind = 5\n",
    "reference_event_id = all_event_ids[ref_ind]\n",
    "\n",
    "# Select comparison events (e.g., every 10th event for next 100 events)\n",
    "comparison_event_ids = all_event_ids[ref_ind+1::3]  # Events 1, 11, 21, 31, ... 91\n",
    "\n",
    "print(f\"Reference event: {reference_event_id}\")\n",
    "print(f\"Comparison events: {len(comparison_event_ids)}\")\n",
    "\n",
    "# Run with QC filters\n",
    "results_df, qc_stats, ref_info = analyze_temporal_evolution_robust(\n",
    "    comb_stZ, df_picks,\n",
    "    reference_event_id=reference_event_id,\n",
    "    comparison_event_ids=comparison_event_ids,\n",
    "    pulse_window_start=0.01,\n",
    "    pulse_window_end=0.04,\n",
    "    stretch_max=0.50,\n",
    "    calculate_spectral_fc=True,\n",
    "    min_stretch_cc=0.5,      # Adjust these thresholds as needed\n",
    "    min_alignment_cc=0.50,\n",
    "    min_snr=3.0\n",
    ")\n",
    "\n",
    "# Plot (show rejected events as faded)\n",
    "plot_temporal_evolution_robust(results_df, ref_info, show_rejected=True)\n",
    "\n",
    "# Filter to only QC-passed events\n",
    "results_qc_pass = results_df[results_df['qc_pass'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ade63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic analysis - run AFTER analyze_temporal_evolution_robust completes\n",
    "# Uses: results_df, qc_stats from your analysis\n",
    "\n",
    "print(\"QC Failure Breakdown:\")\n",
    "print(f\"Total events: {len(results_df)}\")\n",
    "print(f\"\\nQC Status:\")\n",
    "print(results_df['qc_pass'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QC Metrics Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get the thresholds you used (adjust these to match what you actually ran with)\n",
    "min_stretch_cc = 0.75\n",
    "min_alignment_cc = 0.70\n",
    "min_snr = 3.0\n",
    "\n",
    "# Check each metric\n",
    "print(\"\\nStretch CC:\")\n",
    "print(f\"  Min: {results_df['stretch_cc'].min():.3f}\")\n",
    "print(f\"  Max: {results_df['stretch_cc'].max():.3f}\")\n",
    "print(f\"  Mean: {results_df['stretch_cc'].mean():.3f}\")\n",
    "print(f\"  Median: {results_df['stretch_cc'].median():.3f}\")\n",
    "print(f\"  Below threshold ({min_stretch_cc}): {(results_df['stretch_cc'] < min_stretch_cc).sum()}\")\n",
    "\n",
    "print(\"\\nAlignment CC:\")\n",
    "print(f\"  Min: {results_df['mean_alignment_cc'].min():.3f}\")\n",
    "print(f\"  Max: {results_df['mean_alignment_cc'].max():.3f}\")\n",
    "print(f\"  Mean: {results_df['mean_alignment_cc'].mean():.3f}\")\n",
    "print(f\"  Median: {results_df['mean_alignment_cc'].median():.3f}\")\n",
    "print(f\"  Below threshold ({min_alignment_cc}): {(results_df['mean_alignment_cc'] < min_alignment_cc).sum()}\")\n",
    "\n",
    "print(\"\\nMean SNR:\")\n",
    "print(f\"  Min: {results_df['mean_snr'].min():.1f}\")\n",
    "print(f\"  Max: {results_df['mean_snr'].max():.1f}\")\n",
    "print(f\"  Mean: {results_df['mean_snr'].mean():.1f}\")\n",
    "print(f\"  Median: {results_df['mean_snr'].median():.1f}\")\n",
    "print(f\"  Below threshold ({min_snr}): {(results_df['mean_snr'] < min_snr).sum()}\")\n",
    "\n",
    "# Show histogram of QC metrics\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(results_df['stretch_cc'].dropna(), bins=30, edgecolor='k')\n",
    "ax.axvline(min_stretch_cc, color='r', linestyle='--', linewidth=2, label=f'Threshold = {min_stretch_cc}')\n",
    "ax.set_xlabel('Stretch CC')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Stretch Correlation Coefficient')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(results_df['mean_alignment_cc'].dropna(), bins=30, edgecolor='k')\n",
    "ax.axvline(min_alignment_cc, color='r', linestyle='--', linewidth=2, label=f'Threshold = {min_alignment_cc}')\n",
    "ax.set_xlabel('Alignment CC')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Mean Alignment CC')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.hist(results_df['mean_snr'].dropna(), bins=30, edgecolor='k')\n",
    "ax.axvline(min_snr, color='r', linestyle='--', linewidth=2, label=f'Threshold = {min_snr}')\n",
    "ax.set_xlabel('Mean SNR')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Mean SNR')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look at the QC failure reasons\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Most Common QC Failure Reasons:\")\n",
    "print(\"=\"*60)\n",
    "failed = results_df[results_df['qc_pass'] == False]\n",
    "if len(failed) > 0:\n",
    "    print(failed['qc_reasons'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"No failures!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350815c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
